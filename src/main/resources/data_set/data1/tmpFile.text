简介 word 2 vec   是   google   在   2013   年 年 中 开 源 的 一款 将 词 表征 为 实数 值 向 量 的 高效 工具 ,   其 利 用 深度 学习 的 思想 ， 可以 通过 训练 ， 把 对 文本 内容 的 处理 简化 为   k   维 向量 空间 中 的 向量 运算 ， 而 向量 空间 上 的 相似 度 可以 用 来 表示 文本 语义 上 的 相似 度 。 word 2 vec 输出 的 词 向量 可以 被 用 来 做 很多   nlp   相关 的 工作 ， 比如 聚 类 、 找 同义词 、 词性 分析 等等 。 如果 换 个 思路 ，   把 词 当做 特征 ， 那么 word 2 vec 就 可以 把 特征 映射 到   k   维 向量 空间 ， 可以 为 文本 数据 寻求 更加 深 层次 的 特征 表示   。 word 2 vec   使用 的 是   distributed   representation   的 词 向量 表示 方式 。 distributed   representation   最 早 由   hinton 在   1986   年 提出 [ 4 ] 。 其 基本 思想 是   通过 训练 将 每个 词 映射 成   k   维 实数 向量 （ k   一般 为 模型 中 的 超 参数 ） ， 通过 词 之间 的 距离 （ 比如   cosine   相似 度 、 欧 氏 距离 等 ） 来 判断 它们 之间 的 语义 相似 度 . 其 采用 一个   三层 的 神经 网络   ， 输入 层 - 隐 层 - 输出 层 。 有 个 核心 的 技术 是   根据 词频 用 huffman 编码   ， 使得 所有 词频 相似 的 词 隐藏 层 激 活 的 内容 基本 一致 ， 出现 频率 越 高 的 词语 ， 他们 激 活 的 隐藏 层 数目 越 少 ， 这样 有效 的 降低 了 计算 的 复杂 度 。 而 word 2 vec 大 受 欢迎 的 一个 原因 正 是 其 高效 性 ， mikolov   在 论文 [ 2 ] 中 指出 ， 一个 优化 的 单机 版本 一天 可 训练 上 千亿 词 。 这个 三层 神经 网络 本身 是   对 语言 模型 进行 建 模   ， 但 也 同时   获得 一种 单词 在 向量 空间 上 的 表示   ， 而 这个 副作用 才 是 word 2 vec 的 真正 目标 。 与 潜 在 语义 分析 （ latent   semantic   index ,   lsi ） 、 潜 在 狄立克 雷 分配 （ latent   dirichlet   allocation ， lda ） 的 经典 过程 相比 ， word 2 vec 利用 了 词 的 上下文 ， 语义 信息 更加 地 丰富 。 样 例 实验 在 服务器 上 部署 有 word 2 vec 系统 , 可以 试试 玩 一 玩 简介 word 2 vec   是   google   在   2013   年 年 中 开 源 的 一款 将 词 表征 为 实数 值 向 量 的 高效 工具 ,   其 利 用 深度 学习 的 思想 ， 可以 通过 训练 ， 把 对 文本 内容 的 处理 简化 为   k   维 向量 空间 中 的 向量 运算 ， 而 向量 空间 上 的 相似 度 可以 用 来 表示 文本 语义 上 的 相似 度 。 word 2 vec 输出 的 词 向量 可以 被 用 来 做 很多   nlp   相关 的 工作 ， 比如 聚 类 、 找 同义词 、 词性 分析 等等 。 如果 换 个 思路 ，   把 词 当做 特征 ， 那么 word 2 vec 就 可以 把 特征 映射 到   k   维 向量 空间 ， 可以 为 文本 数据 寻求 更加 深 层次 的 特征 表示   。 word 2 vec   使用 的 是   distributed   representation   的 词 向量 表示 方式 。 distributed   representation   最 早 由   hinton 在   1986   年 提出 [ 4 ] 。 其 基本 思想 是   通过 训练 将 每个 词 映射 成   k   维 实数 向量 （ k   一般 为 模型 中 的 超 参数 ） ， 通过 词 之间 的 距离 （ 比如   cosine   相似 度 、 欧 氏 距离 等 ） 来 判断 它们 之间 的 语义 相似 度 . 其 采用 一个   三层 的 神经 网络   ， 输入 层 - 隐 层 - 输出 层 。 有 个 核心 的 技术 是   根据 词频 用 huffman 编码   ， 使得 所有 词频 相似 的 词 隐藏 层 激 活 的 内容 基本 一致 ， 出现 频率 越 高 的 词语 ， 他们 激 活 的 隐藏 层 数目 越 少 ， 这样 有效 的 降低 了 计算 的 复杂 度 。 而 word 2 vec 大 受 欢迎 的 一个 原因 正 是 其 高效 性 ， mikolov   在 论文 [ 2 ] 中 指出 ， 一个 优化 的 单机 版本 一天 可 训练 上 千亿 词 。 这个 三层 神经 网络 本身 是   对 语言 模型 进行 建 模   ， 但 也 同时   获得 一种 单词 在 向量 空间 上 的 表示   ， 而 这个 副作用 才 是 word 2 vec 的 真正 目标 。 与 潜 在 语义 分析 （ latent   semantic   index ,   lsi ） 、 潜 在 狄立克 雷 分配 （ latent   dirichlet   allocation ， lda ） 的 经典 过程 相比 ， word 2 vec 利用 了 词 的 上下文 ， 语义 信息 更加 地 丰富 。 样 例 实验 在 服务器 上 部署 有 word 2 vec 系统 , 可以 试试 玩 一 玩 简介 word 2 vec   是   google   在   2013   年 年 中 开 源 的 一款 将 词 表征 为 实数 值 向 量 的 高效 工具 ,   其 利 用 深度 学习 的 思想 ， 可以 通过 训练 ， 把 对 文本 内容 的 处理 简化 为   k   维 向量 空间 中 的 向量 运算 ， 而 向量 空间 上 的 相似 度 可以 用 来 表示 文本 语义 上 的 相似 度 。 word 2 vec 输出 的 词 向量 可以 被 用 来 做 很多   nlp   相关 的 工作 ， 比如 聚 类 、 找 同义词 、 词性 分析 等等 。 如果 换 个 思路 ，   把 词 当做 特征 ， 那么 word 2 vec 就 可以 把 特征 映射 到   k   维 向量 空间 ， 可以 为 文本 数据 寻求 更加 深 层次 的 特征 表示   。 word 2 vec   使用 的 是   distributed   representation   的 词 向量 表示 方式 。 distributed   representation   最 早 由   hinton 在   1986   年 提出 [ 4 ] 。 其 基本 思想 是   通过 训练 将 每个 词 映射 成   k   维 实数 向量 （ k   一般 为 模型 中 的 超 参数 ） ， 通过 词 之间 的 距离 （ 比如   cosine   相似 度 、 欧 氏 距离 等 ） 来 判断 它们 之间 的 语义 相似 度 . 其 采用 一个   三层 的 神经 网络   ， 输入 层 - 隐 层 - 输出 层 。 有 个 核心 的 技术 是   根据 词频 用 huffman 编码   ， 使得 所有 词频 相似 的 词 隐藏 层 激 活 的 内容 基本 一致 ， 出现 频率 越 高 的 词语 ， 他们 激 活 的 隐藏 层 数目 越 少 ， 这样 有效 的 降低 了 计算 的 复杂 度 。 而 word 2 vec 大 受 欢迎 的 一个 原因 正 是 其 高效 性 ， mikolov   在 论文 [ 2 ] 中 指出 ， 一个 优化 的 单机 版本 一天 可 训练 上 千亿 词 。 这个 三层 神经 网络 本身 是   对 语言 模型 进行 建 模   ， 但 也 同时   获得 一种 单词 在 向量 空间 上 的 表示   ， 而 这个 副作用 才 是 word 2 vec 的 真正 目标 。 与 潜 在 语义 分析 （ latent   semantic   index ,   lsi ） 、 潜 在 狄立克 雷 分配 （ latent   dirichlet   allocation ， lda ） 的 经典 过程 相比 ， word 2 vec 利用 了 词 的 上下文 ， 语义 信息 更加 地 丰富 。 样 例 实验 在 服务器 上 部署 有 word 2 vec 系统 , 可以 试试 玩 一 玩 简介 word 2 vec   是   google   在   2013   年 年 中 开 源 的 一款 将 词 表征 为 实数 值 向 量 的 高效 工具 ,   其 利 用 深度 学习 的 思想 ， 可以 通过 训练 ， 把 对 文本 内容 的 处理 简化 为   k   维 向量 空间 中 的 向量 运算 ， 而 向量 空间 上 的 相似 度 可以 用 来 表示 文本 语义 上 的 相似 度 。 word 2 vec 输出 的 词 向量 可以 被 用 来 做 很多   nlp   相关 的 工作 ， 比如 聚 类 、 找 同义词 、 词性 分析 等等 。 如果 换 个 思路 ，   把 词 当做 特征 ， 那么 word 2 vec 就 可以 把 特征 映射 到   k   维 向量 空间 ， 可以 为 文本 数据 寻求 更加 深 层次 的 特征 表示   。 word 2 vec   使用 的 是   distributed   representation   的 词 向量 表示 方式 。 distributed   representation   最 早 由   hinton 在   1986   年 提出 [ 4 ] 。 其 基本 思想 是   通过 训练 将 每个 词 映射 成   k   维 实数 向量 （ k   一般 为 模型 中 的 超 参数 ） ， 通过 词 之间 的 距离 （ 比如   cosine   相似 度 、 欧 氏 距离 等 ） 来 判断 它们 之间 的 语义 相似 度 . 其 采用 一个   三层 的 神经 网络   ， 输入 层 - 隐 层 - 输出 层 。 有 个 核心 的 技术 是   根据 词频 用 huffman 编码   ， 使得 所有 词频 相似 的 词 隐藏 层 激 活 的 内容 基本 一致 ， 出现 频率 越 高 的 词语 ， 他们 激 活 的 隐藏 层 数目 越 少 ， 这样 有效 的 降低 了 计算 的 复杂 度 。 而 word 2 vec 大 受 欢迎 的 一个 原因 正 是 其 高效 性 ， mikolov   在 论文 [ 2 ] 中 指出 ， 一个 优化 的 单机 版本 一天 可 训练 上 千亿 词 。 这个 三层 神经 网络 本身 是   对 语言 模型 进行 建 模   ， 但 也 同时   获得 一种 单词 在 向量 空间 上 的 表示   ， 而 这个 副作用 才 是 word 2 vec 的 真正 目标 。 与 潜 在 语义 分析 （ latent   semantic   index ,   lsi ） 、 潜 在 狄立克 雷 分配 （ latent   dirichlet   allocation ， lda ） 的 经典 过程 相比 ， word 2 vec 利用 了 词 的 上下文 ， 语义 信息 更加 地 丰富 。 样 例 实验 在 服务器 上 部署 有 word 2 vec 系统 , 可以 试试 玩 一 玩 